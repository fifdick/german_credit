---
title: "RChallenge German credit dataset"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
    self_contained: true
    code_folding: hide
---

```{r}
knitr::opts_chunk$set(collapse = TRUE, echo = FALSE, message = FALSE, warning = FALSE)
```


```{r echo =  FALSE}
set.seed(123)
library(knitrBootstrap)
library(rchallenge)
library(DT)
library(dplyr)
rawdata <- rchallenge::german
```

# General data exploration 

Raw, unedited data looks messy:

```{r}
ncol(rawdata)
DT::datatable(rawdata)
```

Plan: encode, introduce dummy variables where necessary, data split, .

```{r}
#mutating numerical columns to numeric  
rawdata %>% 
	mutate(age = as.numeric(age),
	       amount = as.numeric(amount),
	       duration = as.numeric(duration))-> rawdata
# Exploring factor variables
columninfo <- apply(rawdata %>% select(!where(is.numeric)), 2, function(col) {
col %>% table -> mystr
mystr	      #cat(col, "\n", as.character(mystr), "\n")
	  })

columninfo
```

```{r}
edited <- rawdata %>%
	mutate(status = 
	       case_when(status == "... < 0 DM" ~
```

## Data split


## Missing values

```{r}
apply(rawdata, 2, is.character)
rawdata %>% is.na %>% colSums
```

No missing values? 

## Response variable

Data leak if distribution of classes is investigated on complete dataset?

```{r}
rawdata %>% group_by(credit_risk) %>% tally
```

Notes:
outlier detection scatter plots
paralllel category plot to relate categorical variables to each other
-brier score to check if predicted propabilities match actual probabilties
-feature importance to interpretation
e.g information gain xgboost |
30% bad cases-> if are the positives (1) then FN are very important that would be missed bad risks
if good are the positives then FP are very important..that encoding is more intuitive -> ROC AUC but maybe better PR AUC for both classes  and F1 score hamonic average--> detection of rare events but they dont generalize well
does no checking or saving account most likley mean no savings or is it a true NA
-standard scale numerical variables (robust to outliers) 
-age categories?!
-split personal status and gender?
-number of existing credits is ordinal
-which other variables are oridnal?
	people liable
-imbalanced predictors:
	foreign workers
	guarantors
	other installment plans
	job unemployed category
	number of existing credits category 4
	personal status
	checking account
	--> could  reduce number of categories for these
-simple feature selection: chi square
-pca?
-balance dataset:
	by definition mild imbalance (within 40% minority cases)
	balance train:
	1. train on true distribution -> if it generalizes well good
	2. downsample balance training data--> reduce number of bad risk instances
	   stratified sampling versus random sampling
	   Accuracy bad metric for imbalanced dataset
	   area under ROC
	   so try to model without resampling but look at precision and recall and if at accuracy then compare to baseline
	SMOTE in python synthesizes minority class samples
-zeroR or randomRate
Odds of Guessing Minority Correct: 0.25 * 0.25 = 0.0625
Odds of Guessing Majority Correct: 0.75 * 0.75 = 0.5625
Baseline = 0.25**2 + 0.75**2 = 0.625
-> informs how much value model adds to random guess but accuracy must additionally be over majority vote guess (zeroR)

-test 25%
-caret createDataPartition, DMwR -> smote function
trainIndex <- createDataPartition(iris$Species, p = .8, list = FALSE)
training <- iris[trainIndex, ]
testing <- iris[-trainIndex, ]
# Apply SMOTE to the training set
training_balanced <- SMOTE(Species ~ ., training, k = 5, perc.over = 100, perc.under = 200)

# Train a decision tree model on the resampled training set
model <- train(Species ~ ., training_balanced, method = "rpart")

# Evaluate the performance of the trained model on the testing set
predictions <- predict(model, testing)
confusionMatrix(predictions, testing$Species)
-feature selection
