---
title: "RChallenge German credit dataset"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
    self_contained: true
    code_folding: hide
---

```{r}
knitr::opts_chunk$set(collapse = TRUE, echo = TRUE, message = FALSE, warning = FALSE)
```


```{r echo =  FALSE}
set.seed(123)
library(knitrBootstrap)
library(rchallenge)
library(DT)
library(dplyr)
library(ggplot2)
library(reshape2)
library(cowplot)
library(patchwork)
library(rstatix)
rawdata <- rchallenge::german
#define colors
cols_sex = c("F" = "lightseagreen", "M" = "violetred4")
cols_risk = c("good" = "darkseagreen3", "bad" = "firebrick2")
```

# General data exploration 

Raw, unedited data looks messy:

```{r echo = FALSE}
dim(rawdata)
rawdata <- rawdata %>%	
	mutate(gender = case_when(personal_status_sex == "male : divorced/separated" | personal_status_sex == "male : married/widowed" ~ "M",
			          personal_status_sex == "female : non-single or male : single" ~ NA,
				  personal_status_sex == "female : single" ~ "F")) 

DT::datatable(rawdata)
```

## Response variable 

- (Should the data be split before EDA or after? Performing associations between response variable and predictors could result in "data leak".)   
- The response variable is imbalanced with the minority class being the one that is "more important" since we aim at identifying bad risk individuals.  
- By definition this imbalance is still mild. Therefore I conclude to not think about downsampling but rather making sure to use an adequate evaluation metric (not accuracy) later on.   
```{r echo = FALSE}
rawdata %>%
	group_by(credit_risk) %>%
	tally %>%
	mutate(Perc = paste0(100*n / sum(n), "%"))
```

## Imbalances among predictor variables

Skewed distributions or categorical variables with underrepresented categories?

```{r fig.height = 9}
#mutating numerical columns to numeric  
rawdata %>% 
	mutate(age = as.numeric(age),
	       amount = as.numeric(amount),
	       duration = as.numeric(duration))-> rawdata
rawdata %>% mutate(id = as.factor(seq(1,nrow(.)))) %>%
	select(id, credit_risk, where(is.numeric)) %>%
#	mutate(Age_logged = log2(age),
#	       Duration = log2(duration),
#	       Loan_amount = log2(amount)) %>%
	reshape2::melt(., id.vars = c("id", "credit_risk")) %>%
	mutate(variable = case_when(variable == "age" ~ "Age",
				    variable == "duration" ~ "Duration",
				    variable == "amount" ~ "Loan amount")) -> df
ggplot(df, aes(x = value)) +
	geom_histogram(fill = "transparent", col = "black", aes(y = ..density..), bins = 40) +
	geom_density() +
	facet_wrap(~variable, ncol = 1, scales = "free") +
	cowplot::theme_cowplot() -> p1
p1
ggplot(df, aes(x = credit_risk, y = value, col = credit_risk)) +
	#geom_histogram(bins = 40, fill = "transparent", aes( y = ..density..)) +
	#geom_density() +
	geom_violin() +
 	geom_boxplot(width = .2) +
	scale_color_manual(values = cols_risk) +
	ggpubr::stat_compare_means() +
	#scale_fill_manual(values = cols_risk) +
	facet_wrap(~variable, ncol = 1, scales = "free") +
	cowplot::theme_cowplot() -> p2
p2
 df %>% 
	 tidyr::pivot_wider(., names_from = "variable", id_cols = c("id", "credit_risk"), values_from = "value") -> df_wide
 df_wide %>% 
	 select(where(is.numeric)) %>%
	cor(., use = "pairwise", method = "spearman") %>%
	ggcorrplot::ggcorrplot(., hc.order = T, outline.col = "white", lab = T) +
	 theme(axis.text.x = element_text(margin=margin(-2,0,0,0)),  # Order: top, right, bottom, left
        axis.text.y = element_text(margin=margin(0,-2,0,0))) +
  	geom_vline(xintercept=1:3-0.5, colour="white", size=2) +
  	geom_hline(yintercept=1:3-0.5, colour="white", size=2)  -> p3

p3
# Can numerical variables alone separate good from bad?
prcomp(df_wide %>% select(where(is.numeric)) %>% log, scale = T, center = T) -> pca 
# Extract loadings of the variables
pca_loadings <- data.frame(Variables = rownames(pca$rotation), pca$rotation)
pca$x %>%
	as.data.frame %>%
	mutate(id = rownames(.)) %>%
	left_join(df_wide, by = "id") %>%
	ggplot(., aes(x = PC1, y = PC2, col = credit_risk)) +
	geom_point() +
	scale_color_manual(values = cols_risk) +
	stat_ellipse() +
	geom_segment(data = pca_loadings, aes(x = 0, y = 0, xend = (PC1),
     yend = (PC2)), arrow = arrow(length = unit(1, "picas")),
     color = "black") +
	annotate("text", x = (pca_loadings$PC1), y = (pca_loadings$PC2),
     label = pca_loadings$Variables) +
	theme_cowplot()


#(p1 + p2) / p3
```
- Loan amount and Duration both along the first, explaining most of the variance, age in the second PC
- Log transfrom to alleviat skewedness AFTER data split
- identificaton of outliers after datasplit replot distributions?
- Corrplot for categorical variables

```{r cat_gender,  fig.width = 15, fig.height = 12}
rawdata %>%
	select(-where(is.numeric)) %>%
	mutate(id = seq(1,nrow(.))) %>%
	reshape2::melt(., id.vars = c("gender", "id")) %>% 
	group_by(variable, gender, value) %>%
	tally %>%
	mutate(perc = 100 * (n / sum(n))) %>% 
ggplot(., aes(x = value, y = perc, fill = gender)) +
geom_bar(position = "dodge", stat = "identity") +
coord_flip() +
scale_fill_manual(values = cols_sex) +
facet_wrap(~variable, ncol = 3, scales =  "free") +
theme_cowplot()
```

```{r cat_risk, fig.width = 15, fig.height = 12}
rawdata %>%
	select(-where(is.numeric)) %>%
	mutate(id = seq(1,nrow(.))) %>%
	reshape2::melt(., id.vars = c("credit_risk", "id")) %>% 
	group_by(variable, credit_risk, value) %>%
	tally %>%
	mutate(perc = 100 * (n / sum(n))) %>% 
ggplot(., aes(x = value, y = perc, fill = credit_risk)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
scale_fill_manual(values = cols_risk) +
facet_wrap(~variable, scales =  "free", ncol = 3) +
theme_cowplot()
```

NEED TO THINK ABOUT WHAT PERCENTAGE TO SHOW IN THE PLOT:
how many bad/good in one level or how much percentage a level makes up of the category

Note: now im only looking at associations with credit risk but what baout correlations between predictors?

# Feature engineering 


No missing values, although in truth there is missing information (e.g. people without checking or savings account). But no imputation needed right now.

```{r }
sum(rawdata %>% is.na %>% colSums) > 0
```

Identify categorical variables and print levels to get an overview

```{r result = "asis"}
# Exploring factor variables
columninfo <- apply(rawdata %>% select(!where(is.numeric)), 2, function(col) {
col %>% table -> mystr
mystr	      #cat(col, "\n", as.character(mystr), "\n")
	  })

#columninfo
```

Notes and thoughts while encoding variables. Some decisions are probably not optimal but time is limited.  

- Encode categorical variables which are ordinal with integers
- Introduce dummy variables for true categorical variables
- Encoding the response variable such that the positive class (1) is assigned to "bad" risk: 
	-->more intuitive because we want to avoid missing bad risk cases (FN), rather than missing good risk cases.  
- The encoding code be done in a much simpler way but now I will also directly go through the variables to get familiar with them  
- For some variables it is difficult for me to know if they could actually be oridnal, like housing or "other installment plans", so I will keep them categorical. And of course the point is to identify possibly new insights, for example that after all people who have life insurances are a lower risk than people with real estate  
- Im splitting personal status and gender because Im interested if one of these will come up higher later in the variable importance analysis --> hm cant actually split it, weird variable  
- I assume relativeley equal magnitudes between the values of those categorical ordinal variables, although this could be argued about. 


```{r encoding}
edited <- rawdata %>%
	mutate(credit_risk = ifelse(credit_risk == "bad", 1, 0), 
	       foreign_worker = ifelse(foreign_worker == "yes", 1, 0),
		telephone = ifelse(telephone == "no", 0, 1),
		people_liable = ifelse(people_liable == "0 to 2", 1, 2),
		number_credits = case_when( number_credits == "1" ~ 1,
					    number_credits == "2-3" ~ 2,
					    number_credits == "4-5" ~ 3,
					    number_credits == ">= 6" ~ 4,
					    TRUE ~ NA),
	       present_residence = case_when( present_residence == "< 1 yr" ~ 1,
					      present_residence == "1 <= ... < 4 yrs" ~ 2,
					      present_residence == "4 <= ... < 7 yrs" ~ 3,
					      present_residence == ">= 7 yrs" ~ 4,
					      TRUE ~ NA),
	       installment_rate = case_when( installment_rate == "< 20" ~ 1,
					     installment_rate == "20 <= ... < 25" ~ 2,
					     installment_rate == "25 <= ... < 35" ~ 3,
					     installment_rate == ">= 35" ~ 4,
					     TRUE ~ NA),
	       employment_duration = case_when( employment_duration == "unemployed" ~ 0,
					        employment_duration == "< 1 yr" ~ 1,
						employment_duration == "1 <= ... < 4 yrs" ~ 2,
						employment_duration == "4 <= ... < 7 yrs" ~ 3,
						employment_duration == ">= 7 yrs" ~ 4,
						TRUE ~ NA),
# This is tricky because the case of unknown and no savings account are not necessarily the same. First intuition is to rate it as the lowest cateogory but it could also be a NA, the problem is its 600 values..Other option is to leave this variable categorical even though it is clearly ordinal
# The same applies to status (of checking account)
	 	savings = case_when( savings == "unknown/no savings account" ~ 0,
				     savings == "... <  100 DM" ~  1,
				     savings == "100 <= ... <  500 DM" ~ 2,
				     savings == "500 <= ... < 1000 DM" ~ 3,
				     savings == "... >= 1000 DM" ~ 4,
				     TRUE ~ NA),
	# Treating debt (< 0DM) and no checking account as the same...probably not a good idea
	       status = case_when(status == "... < 0 DM" | status ==  "no checking account" ~ 0,
				   status ==  "0<= ... < 200 DM" ~ 1,
				   status == "... >= 200 DM / salary for at least 1 year" ~ 2)) %>%
	dplyr::bind_cols(model.matrix( ~ 0 + job + housing + other_installment_plans + property + other_debtors + personal_status_sex + purpose + credit_history, rawdata) %>% as.data.frame) 
```

```{r, include=FALSE}
knitr::knit_hooks$set(output = function(x, options){
  if(!is.null(options$max_height)){
    paste('<pre style = "max-height:', options$max_height, '; float: left; width: 700px; overflow-y: auto;">', x, "</pre>", sep = "")
  }else{
    x
  }
  
})
```

Now we have `r ncol(edited)-1` variables: 

```{r echo = FALSE, max_height = "100px"}
print(colnames(edited))
```

## Data split

random split or stratified?
down sampling or not?


Consider transformations or scaling of numerical variables (depends on distribution and algorithm to be used on the data). Not doing anything for now due to time reasons (and the need to read up on it)

```{r scaling}
#log numerical variables
```




